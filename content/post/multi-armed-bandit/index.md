+++
date = "2019-03-11"
title = "Многорукий бандит"
categories = [ "Алгоритмы" ]
tags = [ "бандит", "алгоритм", "тестирование" ]
draft = true

[image]
  caption = "Изображение: Pixel Privacy"
  focal_point = "Smart"
  preview_only = false
+++

## План

- Описание математики простыми словами
- Практический пример с баннерами на сайте

Перевод статьи [The Multi-Armed Bandit Problem and Its Solutions](https://lilianweng.github.io/lil-log/2018/01/23/the-multi-armed-bandit-problem-and-its-solutions.html)

Проблема многорукого бандита - классический пример для демонстрации дилеммы исследования против использования. Эта статья введение в проблему бандита. Расскажем как ее решать используя различные стратегии исследования.

Реализация алгоритма бандита Бернулли [доступна на github](https://github.com/lilianweng/multi-armed-bandit)

## Использование против исследования 

Проблема использования против исследования существует во многих аспектах нашей жизни. Представим, что ваш любимый ресторан за углом. Если вы будете ходить туда каждый вечер, то точно будете получать удовольствие. Но в этом случае, вы теряете шанс получить больше удовольствия в другом месте. Но если ходить в новое место каждый день, то вероятно вы иногда будете есть не вкусную еду и получать меньше удовольствия чем в знакомом месте. Похожим образом приходится балансировать между старыми привлекательными объявлениями и новыми, более выгодными в перспективе.

![Рис.1 Пример дилеммы исследования против использования в реальной жизни: где сегодня поесть?](/img/multi-armed-bandit/exploration_vs_exploitation.png)

Если у нас будет вся информация об окружающей среде, то найти лучшую стратегию можно грубой силой и обычным брутфорсом. Дилемма начинается, когда информации мало: нужно собрать достаточно информации для принятия решения, при этом учитывать риски. При использовании мы выбираем лучший вариант из уже известных. При исследовании мы собираем информацию о неизвестных вариантах и рискуем эффективностью. Лучшая стратегия учитывает краткосрочные потери. Например, одно из исследований оказалось полным провалом, но значит в будущем мы не будем пользоваться этим вариантом.

## Что такое многорукий бандит?

Многорукий бандит - пример классической дилеммы использования против исследования. Представьте что вы играете в казино на нескольких автоматах у которых по разному настроена вероятность выигрыша. Возникает вопрос: какая наилучшая стратегия, чтобы максимизировать выигрыш на протяжении долгого времени?

В этой статье мы будем обсуждать только случай когда у нас есть бесконечное число попыток для тестирования. Ограничение на число таких попыток привод к новому типу задач. Например, если число попыток будет меньше чем число игровых автоматов, то естественно мы не сможем оценить вероятность вознаграждения каждого автомата. В этом случае нужно 
более разумно использовать ресурсы.

![Рис.2 Иллюстрация работы многорукого бандита Бернулли. Вероятности вознаграждения игроку не известны.](/img/multi-armed-bandit/bern_bandit.png)

Наивный подход - играть за одним автоматом много раундов, чтобы оценить вероятность вознаграждения по закону больших чисел. Но это очень расточительно и не гарантирует лучший выигрыш.

### Определение

Теперь дадим научное определение.

Многорукий бандит Бернули можно описать как кортеж $\left \langle A, R \right \rangle$

* У нас есть $K$ игровых автоматов с вероятностями выигрыша $\left \\{ \Theta _{1}, ...,\Theta _{n} \right \\}$
* За один временной шаг t мы играем на одном автомате и получаем выигрыш r.
* $A$ это набор действий с каждым игровым автоматом. Каждое действие a имеет некоторый ожидаемый результат $Q(a) = \mathbb{E}[r|a] = \Theta $. Если действие $a \_{t}$ выполнено в момент времени t, то ожидаемый результат этого действия можно представить как $Q(a \_{t}) = \Theta \_{i}$.
* $R$ это функция выигрыша. В случае с бандитом Бернули, вознаграждение r мы определяем стохастическим образом. В шаг времени t, $ r \_{t} = R(a \_{t}) $ будет равно 1 с вероятностью $ Q(a\_{t}) $ или 0.

Это упрощенная версия [Марковского процесса принятия решений](https://bit.ly/1EtOqPO) - у нас нет состояния S.

Наша цель - увеличить общий выигрыш  $\sum\_{t=1}^{T}r\_{1}$. Если мы знаем результаты всех наших дерганий за ручку, то наша задача - уменьшить потенциальный проигрыш не выбирая каждый раз оптимальное действие.

Лучшая вероятность вознаграждения $\Theta^{\*}$ при оптимальном действии $a^{\*}$:

$$ \Theta^{\*} = Q(a^{\*}) = \max\_{a \epsilon A}  Q(a) = \max\_{1 \leqslant i \leqslant k} \Theta\_{1} $$

Функция потерь - это весь проигрыш который ждет нас, если мы не будем выбирать оптимальное действие на каждом шаге T:

$$ \large{\iota}\_{T} = \mathbb{E}\left [ \sum\_{t=1}^{T}(\Theta^{\*} - Q(a\_{t})) \right ] $$

### Стратегии

Есть три подхода для решения проблемы многорукого бандита

* Ничего не исследовать: самый наивные и плохой подход.
* Исследовать рандомно.
* Исследовать разумна с предпочтительной неопределенностью.

## ε-Жадный алгоритм

ε-жадный алгоритм большую часть времени выполняет лучшее действие, но иногда делает случайное исследование. Для оценки результативности действия используется усредненная выгода от всех действий сделанных до этого момента.

$$ \widehat{Q}\_{t}(a)=\frac{1}{N\_{t}(a)}\sum\_{\tau=1}^{t}r\_{\tau}1[a\_{\tau} = a] $$

В этой формуле 1 - это функция бинарный индикатор. $ N\_{t}(a) $ - сколько раз действие $ a $ было выбрано. Таким образом $ N\_{t}(a) = \sum\_{\tau=1}^{t}1[a\_{\tau} = a] $

При использовании ε-жадного алгоритма мы выполняем случайное действие с маленькой вероятностью $ \epsilon  $. Но почти все остальное время(с вероятностью $ 1 - \epsilon $) выбирается лучшее действие из тех что были на этот момент: $ \widehat{a}\_{t}^*= \arg \max_{a \epsilon A} \widehat{Q}\_{t}(a) $

Реализация алгоритма [доступна на гитхабе](https://github.com/lilianweng/multi-armed-bandit/blob/master/solvers.py#L45)

## Верхние доверительные границы

Случайные исследования дают нам возможность попробовать действия о которых мы ничего не знаем. К сожалению, при случайном выборе могут попадаться действия, про которые мы уже знаем, что они мало результативные. Для решения этой проблемы используют два подхода. Можно с течением времени уменьшать параметр ε. Или использовать оптимистический подход для действий с высокой неопределенностью - это позволит чаще выбирать действия для которых еще нет уверенной оценки результативности. Другими словами, мы предпочитаем исследовать действия с большим потенциалом для объективной оценки.



## Ссылки

* https://www.spotx.tv/resources/blog/developer-blog/introduction-to-multi-armed-bandits-with-applications-in-digital-advertising/
* http://datareview.info/article/6-prostyih-shagov-dlya-osvoeniya-naivnogo-bayesovskogo-algoritma-s-primerom-koda-na-python/
* https://towardsdatascience.com/comparing-multi-armed-bandit-algorithms-on-marketing-use-cases-8de62a851831
* https://medium.com/@congyuzhou/k-armed-%D0%B1%D0%B0%D0%BD%D0%B4%D0%B8%D1%82-%D0%B8-greedy-%D0%B0%D0%BB%D0%B3%D0%BE%D1%80%D0%B8%D1%82%D0%BC-6502891dbae6