+++
date = "2019-03-11"
title = "Многорукий бандит"
categories = [ "Алгоритмы" ]
tags = [ "бандит", "алгоритм", "тестирование" ]
draft = true

[image]
  caption = "Изображение: Pixel Privacy"
  focal_point = "Smart"
  preview_only = false
+++

Перевод статьи [The Multi-Armed Bandit Problem and Its Solutions](https://lilianweng.github.io/lil-log/2018/01/23/the-multi-armed-bandit-problem-and-its-solutions.html)

Проблема многорукого бандита - классический пример для демонстрации дилеммы исследования против использования. Эта статья введение в проблему бандита. Расскажем как ее решать используя различные стратегии исследования.

Реализация алгоритма бандита Бернулли [доступна на github](https://github.com/lilianweng/multi-armed-bandit)

## Использование против исследования 

Проблема использования против исследования существует во многих аспектах нашей жизни. Представим, что ваш любимый ресторан за углом. Если вы будете ходить туда каждый вечер, то точно будете получать удовольствие. Но в этом случае, вы теряете шанс получить больше удовольствия в другом месте. Но если ходить в новое место каждый день, то вероятно вы иногда будете есть не вкусную еду и получать меньше удовольствия чем в знакомом месте. Похожим образом приходится балансировать между старыми привлекательными объявлениями и новыми, более выгодными в перспективе.

![Рис.1 Пример дилеммы исследования против использования в реальной жизни: где сегодня поесть?](/img/multi-armed-bandit/exploration_vs_exploitation.png)

Если у нас будет вся информация об окружающей среде, то найти лучшую стратегию можно грубой силой и обычным брутфорсом. Дилемма начинается, когда информации мало: нужно собрать достаточно информации для принятия решения, при этом учитывать риски. При использовании мы выбираем лучший вариант из уже известных. При исследовании мы собираем информацию о неизвестных вариантах и рискуем эффективностью. Лучшая стратегия учитывает краткосрочные потери. Например, одно из исследований оказалось полным провалом, но значит в будущем мы не будем пользоваться этим вариантом.

## Что такое многорукий бандит?

Многорукий бандит - пример классической дилеммы использования против исследования. Представьте что вы играете в казино на нескольких автоматах у которых по разному настроена вероятность выигрыша. Возникает вопрос: какая наилучшая стратегия, чтобы максимизировать выигрыш на протяжении долгого времени?

В этой статье мы будем обсуждать только случай когда у нас есть бесконечное число попыток для тестирования. Ограничение на число таких попыток привод к новому типу задач. Например, если число попыток будет меньше чем число игровых автоматов, то естественно мы не сможем оценить вероятность вознаграждения каждого автомата. В этом случае нужно 
более разумно использовать ресурсы.

![Рис.2 Иллюстрация работы многорукого бандита Бернулли. Вероятности вознаграждения игроку не известны.](/img/multi-armed-bandit/bern_bandit.png)

Наивный подход - играть за одним автоматом много раундов, чтобы оценить вероятность вознаграждения по закону больших чисел. Но это очень расточительно и не гарантирует лучший выигрыш.

### Определение

Теперь дадим научное определение.

Многорукий бандит Бернули можно описать как кортеж $\left \langle A, R \right \rangle$

* У нас есть $K$ игровых автоматов с вероятностями выигрыша $\left \\{ \Theta _{1}, ...,\Theta _{n} \right \\}$
* За один временной шаг t мы играем на одном автомате и получаем выигрыш r.
* $A$ это набор действий с каждым игровым автоматом. Каждое действие a имеет некоторый ожидаемый результат $Q(a) = \mathbb{E}[r|a] = \Theta $. Если действие $a \_{t}$ выполнено в момент времени t, то ожидаемый результат этого действия можно представить как $Q(a \_{t}) = \Theta \_{i}$.
* $R$ это функция выигрыша. В случае с бандитом Бернули, вознаграждение r мы определяем стохастическим образом. В шаг времени t, $ r \_{t} = R(a \_{t}) $ будет равно 1 с вероятностью $ Q(a\_{t}) $ или 0.

Это упрощенная версия [Марковского процесса принятия решений](https://bit.ly/1EtOqPO) - у нас нет состояния S.

## Ссылки

* https://www.spotx.tv/resources/blog/developer-blog/introduction-to-multi-armed-bandits-with-applications-in-digital-advertising/
* http://datareview.info/article/6-prostyih-shagov-dlya-osvoeniya-naivnogo-bayesovskogo-algoritma-s-primerom-koda-na-python/
* https://towardsdatascience.com/comparing-multi-armed-bandit-algorithms-on-marketing-use-cases-8de62a851831